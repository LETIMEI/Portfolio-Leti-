# -*- coding: utf-8 -*-
"""「RAG Assignment_final.ipynb」

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bPK3zFUx5pI_sjbBSu0GtoTrSTCLQ8oq

# Retrieval Augmentation Generation

##Packages Installation and Import
"""

# Install Ollama v0.1.30
!curl https://ollama.ai/install.sh | sed 's#https://ollama.ai/download#https://github.com/jmorganca/ollama/releases/download/v0.1.30#' | sh

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Setup the model as a global variable
# OLLAMA_MODEL='phi:latest'
# 
# # Add the model to the environment of the operating system
# import os
# os.environ['OLLAMA_MODEL'] = OLLAMA_MODEL
# !echo $OLLAMA_MODEL # print the global variable to check it saved
# 
# import subprocess
# import time
# 
# # Start ollama on the server ("serve")
# command = "nohup ollama serve&" # "nohup" and "&" means run in the background
# 
# # Use subprocess.Popen to run the command
# process = subprocess.Popen(command,
#                             shell=True,
#                             stdout=subprocess.PIPE,
#                             stderr=subprocess.PIPE)
# 
# time.sleep(5)  # Makes Python wait for 5 seconds
# 
# # Install prerequisites
# !pip install llama-index-embeddings-huggingface
# !pip install llama-index-llms-ollama
# !pip install llama-index ipywidgets
# !pip install llama-index-llms-huggingface
# !pip install llama_index.readers.web
# !pip install llama-index-vector-stores-chroma
# !pip install chromadb
# 
# # Import required modules from the llama_index library
# from llama_index.core import VectorStoreIndex, SummaryIndex, SimpleDirectoryReader
# from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# from llama_index.core import Settings
# from llama_index.llms.ollama import Ollama
# from llama_index.core import StorageContext
# 
# # Import ChromaVectorStore and chromadb module
# from llama_index.vector_stores.chroma import ChromaVectorStore
# import chromadb
# 
# # Import the Ollama class
# from llama_index.llms.ollama import Ollama
# 
# # Use the global variable (OLLAMA_MODEL) as our LLM
# # Set a timeout of 8 minutes in case of CPU
# llm = Ollama(model=OLLAMA_MODEL, request_timeout=480.0)

# Query the model via the command line
# First time running it will "pull" (import) the model

# Test question 1: general question

!ollama run $OLLAMA_MODEL "Give me a comprehensive introduction of the shipping company Yellow Corp."

# Test question 2: specific question

!ollama run $OLLAMA_MODEL "Who were the victim and perpetrator in the murder-suicide incident in Little Egg Harbor, New Jersey?"

# Test question 3: complex question
!ollama run $OLLAMA_MODEL "Why 911 calls for severe allergic reactions nearly doubled in summer? What measures can be taken to prevent serious allergic reaction?"

# Test question 4: question with answer not given in the input context
!ollama run $OLLAMA_MODEL "Who is Emma Stone?"

"""##Data Loading

News data
"""

# data loading

# Install the 'datasets' library from Hugging Face
!pip install datasets

# Import the 'load_dataset' function from the 'datasets' library
from datasets import load_dataset

# Load the 'News_August_2023' dataset from Hugging Face
dataset = load_dataset("RealTimeData/News_August_2023")

import pandas as pd
import re

# Convert the dataset to a pandas DataFrame
df = pd.DataFrame(dataset['train'])

# Display the first few rows of the DataFrame
print(df.head())

df

# Remove duplicate rows based on the 'maintext' column
df = df.drop_duplicates(subset=['maintext'])

# Display the DataFrame after removing duplicates
df

# create an empty directory called "news_data"
!mkdir -p '/content/news_data/'

# Initialize a counter for file naming
count = 0

#store each row in column 'maintext' in separated txt files
for index, row in df.iterrows():
    data_content = row['maintext'] # Get the content of the 'maintext' column for the current row
    fname = "/content/news_data/Output" + str(count) + ".txt"
    with open(fname, "w") as text_file:
        text_file.write(data_content) # Write the content to the text file
    count += 1

"""##Chunking

###Semantic Splitter
"""

# Chunking: semantic splitter
# Load documents from the "/content/news_data" folder
reader = SimpleDirectoryReader("/content/news_data") # load documents from the /data folder
docs = reader.load_data()

# Print the number of documents loaded
print(f"Loaded {len(docs)} docs")

# Initialize a HuggingFace Embedding model
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

# Initialize an LLM model with a specified model and request timeout
llm = Ollama(model=OLLAMA_MODEL, request_timeout=1500.0)

# Specify the LLM and embedding model into LlamaIndex's settings
Settings.llm = llm
Settings.embed_model = embed_model

from llama_index.core.node_parser import SemanticSplitterNodeParser

# Initialize a SemanticSplitterNodeParser with specified parameters
parser = SemanticSplitterNodeParser(
    buffer_size=1, breakpoint_percentile_threshold=90, embed_model=embed_model
)

# Parse the documents into semantic nodes using the parser
semantic_nodes = parser.get_nodes_from_documents(docs)

# Print the semantic nodes for further processing
print(semantic_nodes)

semantic_nodes

# Access the text of the first semantic node
semantic_nodes[0].text

# extract splitted text from the semantic output
all_texts = [node.text for node in semantic_nodes]

# 'all_texts' contains all the extracted texts from each TextNode
all_texts

!mkdir -p '/content/splitted_data/' # create an empty directory called "splitted_data"

count = 0

for doc in all_texts: # iterate through the results
  fname = "/content/splitted_data/Output" + str(count) + ".txt"
  with open(fname, "w") as text_file:
    text_file.write(doc) # save the file
  count += 1 # increment the count

"""##Embedding & Vector Database Setup"""

# Import ChromaVectorStore and chromadb module
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

# Initialize a HuggingFace Embedding model
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

# Initialize an LLM model with a specified model and request timeout
llm = Ollama(model=OLLAMA_MODEL, request_timeout=1500.0)

# Specify the LLM and embedding model into LlamaIndex's settings
Settings.llm = llm
Settings.embed_model = embed_model

# Load documents
reader = SimpleDirectoryReader("/content/splitted_data") # load documents from the /data folder
docs = reader.load_data()
print(f"Loaded {len(docs)} docs")

# Create client ("db") and a database ("chroma_db")
db = chromadb.PersistentClient(path="./chroma_db")

# Create a collection/table in the db
chroma_collection = db.create_collection("my-db")

# Set up ChromaVectorStore and load in data
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
# Specify Chroma as our vector db
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# Create the vector index
vector_index = VectorStoreIndex.from_documents(
    docs, # the file created earlier
    storage_context = storage_context,
    embed_model = embed_model
)

# Print the metadata
print(chroma_collection)

# Print the name of the collection (table)
print(f'Collection name is: {chroma_collection.name}')

"""##Prompt Template Setup"""

# Prompt Template Setup
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.core import ChatPromptTemplate

# Define the QA prompt string
qa_prompt_str = (
    "Below is the context information.\n"
    "---------------------\n"
    "{context_str}\n"
    "---------------------\n"
    "Given the context information and not prior knowledge, "
    "answer the question: {query_str}\n"
)

# Define the text QA prompt messages
chat_text_qa_msgs = [
    ChatMessage(
        role=MessageRole.SYSTEM,
        content=(
            "Please just say 'I don't know' if the answer is not provided in the given context."
        ),
    ),
    ChatMessage(role=MessageRole.USER, content=qa_prompt_str),
]

# Create the ChatPromptTemplate with the defined messages
text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)

"""##Query Testing"""

# Test1: general question
print(
    vector_index.as_query_engine(
        response_mode = 'tree_summarize',
        text_qa_template=text_qa_template,
        llm=llm,
    ).query("Give me a comprehensive introduction of the shipping company Yellow Corp.")
)

# Test2: specific question
print(
    vector_index.as_query_engine(
        response_mode = 'tree_summarize',
        text_qa_template=text_qa_template,
        llm=llm,
    ).query("Who were the victim and perpetrator in the murder-suicide incident in Little Egg Harbor, New Jersey?")
)

# Test3: complex question
print(
    vector_index.as_query_engine(
        response_mode = 'tree_summarize',
        text_qa_template=text_qa_template,
        llm=llm,
    ).query("Why 911 calls for severe allergic reactions nearly doubled in summer? What measures can be taken to prevent serious allergic reaction?")
)

# Test4: not answerable question
print(
    vector_index.as_query_engine(
        response_mode = 'tree_summarize',
        text_qa_template=text_qa_template,
        llm=llm,
    ).query("Who is Emma Stone?")
)